# -*- coding: utf-8 -*-
"""
    all forlura with python
"""

# 向量的范数
#1范数 ||X||1  即向量元素绝对值之和
#2范数 ||X||2  即计算向量长度
from numpy import *
import numpy.linalg as LA  # linalg : numpy中专门用于矩阵科学计算

vector = [1, 2, 3]  # ()表示tuple，不可变; []表示list，可变; {} 表示字典，映射
print(LA.norm(vector,inf)) # 所有向量元素绝对值中的最大值
print(LA.norm(vector,-inf)) # 所有向量元素绝对值中的最小值
print(LA.norm(vector,1)) #1范数  向量元素绝对值之和
print(LA.norm(vector,2)) #2范数  向量元素绝对值的平方和再开方

"""
                                                            矩阵分析
"""

# 矩阵的范数
from numpy import *
import numpy.linalg as LA

matrix = mat('1, 2, 3; 4,5,6; 7,8,9') # mat为numpy中的矩阵
print(LA.norm(matrix,inf)) #无穷范数  矩阵行向量中绝对值之和的最大值
print(LA.norm(matrix,-inf)) 
print(LA.norm(matrix,1)) #1范数   矩阵列向量中绝对值之和的最大值
print(LA.norm(matrix,2)) #2范数   AAt 矩阵的最大特征值的开平方

a=mat([1,2,3])
print("-----------------", LA.norm(a,2)) #2范数   AAt 矩阵的最大特征值的开平方，跟下面的一样，也是欧式集合距离，平方在开方
print("=================", LA.norm(a)) # 默认是2范数

# 矩阵的基本算法
#行列式，方阵才有行列式，表示N个N阶向量所形成的面积，D= |a11, a12; a21, a22| = a11*a22 - a12*a21
det = LA.det(matrix);
print("det is ", det)


# 转置矩阵的性质， (AB).T = B.T*A.T 
#.T表示转置
tmat = matrix.T  
print("matrit.T is ", tmat)

# 逆矩阵 对于n阶方阵A，如果存在n阶方阵C，使得AC=CA=E（E为n阶单位矩阵），则把方阵C称为A的逆矩阵（简称逆阵）记作A-1
#.I表示逆矩阵   (AB)-1 = B-1A-1 
# 矩阵行列式不为0，可逆
imat = matrix.I
print("matrix.I is ", imat)


# 方阵的特征向量和特征值  
# 特征向量在主成分分析（PCA）中具有广泛应用，将维和人脸特征识别
# Ax = vx
# 如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。
# 这个容易理解，可以这样想，把特征向量看成是其他维度值为0，只有某个维度上有值[0,0,3,...,0]，这样一乘只有3这个维度有拉伸，其他不变
# 矩阵意味着变换，对于它的特征向量而言，就是当作用于这个特征向量上时，这个特征向量只会改变长度，而不改变方向
# 特征向量的两个重要的性质1、方向不变形（包括方向的反转） 2、对于振动谱而言，特征向量只改边振幅，而不会产生之后或者超前
a=LA.eigvals(matrix)
print("feature value is ", a)
x=LA.eig(matrix)
print("featurn vector is ", x)

# 向量积（叉积） a^b = c  c为向量，|c| = |a| * |b| * sinθ，c的方向满足右手定律，四指从a弯向b（小于180°）
# 向量积的几何意义为 方向垂直a、b所在平面，长度是以|b|*sinθ为高、|a|为底的平行四边形的面积
# 叉积在图像学中非常有用,通过叉积可以构建XYZ坐标系
# a^b = -b^a
# 


# 数量积（点积） [a,b] = a1*b1 + a2*b2 数量积是一个数，θ=arccos([a,b]/(|a|*|b|)) 
# 数量积 可以用来表征或计算两个向量之间的夹角，以及在b向量在a向量方向上的投影  a.b=|a|.|b|.conθ
# 利用点积可判断一个多边形是否面向摄像机还是背向摄像机
x=[1,2,3]
y=[3,4,5]
z=dot(x,y)
print("dot[x, y] is ", z)

# 
# 正交向量 两个向量的数量积为0
# 

# 正交矩阵 不改变向量间的夹角和长度，不过坐标系做了旋转

# 对称矩阵 元素以主对角线为对称轴对应相等的矩阵 A+A.T为对称矩阵
# 对称矩阵 A为m*n矩阵，A*A.T为对称矩阵
# 对称矩阵用于压缩存储，只需存储上三角信息，能节约一半空间
# 对称矩阵的特征向量是相互正交的

# 相似对角化 将原矩阵化为对角矩阵，且对角矩阵对角线上的每个元素都是原矩阵的特征值

# 特征子空间 有特征向量和零向量组成的空间称为特征子空间。

# SVD奇异值分解 
# 每个矩阵都有奇异值分解，但不一定有特征值分解，特征值分解只有方阵才有
# A = U*D*V.T 其中U和V都是正交矩阵（U*U.T=E）
# SVD 主要用于推荐系统中，数据压缩(因为分解之后，表示一副图像所用的元素变少了)
# 通常奇异值越大对应的信息越有用，能去除冗余信息
# 从几何层面上去理解二维的SVD：对于任意的 2 x 2 矩阵，通过SVD可以将一个相互垂直的网格(orthogonal grid)变换到另外一个相互垂直的网格。
A =mat([[1,2,3],[4,5,6]])
B=LA.svd(A)
print("SVD is ", B)

# 伪逆矩阵 非方阵A 为m*n
# A+ = VD+U.T  对角矩阵D 的伪逆D+ 是其非零元素取倒之后再转置得到的。
B=LA.pinv(A)
print("A+ is ", B)

# 汉明距离 两个字符串中不相同位数的数目。
# 例如：字符串‘1111’与‘1001’之间的汉明距离为2。
matV = mat('1,1,1,1; 1,0,0,1')
smstr = nonzero(matV[0]-matV[1])
print(" distance is ", smstr)


# 杰卡德系数 两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用元素个数来算
# 杰卡德距离 1-杰卡德系数, 用来度量两个集合之间的差异性，取值[0,1] 
# 算法就是   相同元素个数/维数
import scipy.spatial.distance as dist
print("jakade distance is ", dist.pdist(matV,'jaccard'))


"""
                                                                概率分布
在人工智能领域，我们主要以两种方式来使用概率论。
首先，概率法则告诉我们AI系统应该如何推理，
所以我们设计一些算法来计算或者近似由概率论导出的表达式。
其次，我们可以用概率和统计从理论上分析我们提出的AI系统的行为。

"""

# 随机变量 它是对可能的状态的描述，可以是连续或者离散的

# 概率分布 符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性

# 条件概率 某个事件在给定其它事件发生时出现的概率
# p(Y =y| X = x) = P(Y = y, X = x)/P(X = x)

# 贝叶斯公式 也就是后验概率，P(B|A) = P(A|B)*P(B)/P(A)
# 比如：在判断垃圾邮件的算法中:
# P(A) ： 所有邮件中，垃圾邮件的概率。
# P(B) ： 出现某个单词的概率。
# P(B|A) : 垃圾邮件中，出现某个单词的概率。
# P(A|B) : 出现某个单词的邮件，是垃圾邮件的概率。

# 全概率公式 P(A)= P(A|B) + P(A|B-)   P(B)=P(Ai)P(B|Ai)

# 独立事件 P(AB) = P(A)*P(B)
# 互斥事件 即事件A和事件B不可能同时发生
# 对立事件 A U B，A和B互斥， 那么B为A的对立事件

# 事件差 P(A-B)=P(A)-P(AB)
# 加法   P(A U B)=P(A)+P(B)-P(AB)
# 可分   P(A) = P(AB-) + P(AB)


# 几何分布 在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前k-1次皆失败，第k次成功的概率。其概率分布函数为：
# 几何分布 P( X=k) =( 1-p)^{k-1} p

# 二项分布 重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。
# 如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为：
# P( X=k) =Cnk p^{k}( 1-p ) ^{n-k} 

# 自然常数e  e = (x->∞)limit(1+1/x)x  经过推导该函数单调上升，最终收敛无限接近2.71828

# 泊松分布 p(X=k) = e^-λ*λ^k/k! k = 1,2... , r > 0，  λ表示在一定时间（单位时间）内事件发生的平均次数.
# 如果将时间分成很小的时间段，那么该时间片内发生这个事件的概率 p 就会成正比的减少
# 则特定时间段被分成的时间片数量 n 与每个时间片内事件发生的概率 p 的乘积 n*p 为一个常数。这个常数表示了该事件在指定时间段发生的频度
# 在LDA文档主题生成模型分析中应用。即λ的参数估计（单词出现的次数）
# LDA 词通过一定的概率选择了主题，主题通过一定的概率选择了词，LDA是一种非监督学习，
# 泊松分布可以看成是参数为λ的二项分布
# 可以预估这些事件的总数，但是没法知道具体的发生时间。描述某段时间内，事件具体的发生概率

# 分布函数 f(x)
# 单调不减性，f(-∞) = 0; f(+∞) = 1
# 分布函数的概率是积分性质的，连续型概率分布的几何分布 F(x) = ∫x f(u)du ; f(u)为该你密度函数
# 概率密度函数、非负、归一性

# 指数分布 
# 概率密度f(x)= λe^-λx
# 概率分布F(x)=1 - e^-λx

# 正太分布
# f(x,u,σ) = 1/σ√￣2π exp(-(x-u)^2/2σ^2)  ,u为期望，σ为方差
# 密度曲线关于执行x=u对称
# σ越大，曲线越平坦


# 二维随机变量的联合分布
# F(x,y) = P(X<=x, Y<=y)
# 性质 F(-∞, y) = 0  F(x, -∞)=0  F(-∞, +∞)=0

# 边缘概率密度，f(x, y) 当y->∞时，称为F(x) = ∫∞f(x, y)dy 关于x的边缘分布函数
# 随机变量的相互独立性，f(x, y) = f(x) * f(y)

# 二维随机变量的条件概率密度
# F(x|y) = ∫x f(u,v)du/Fy(y)


# 期望 描述随机变量的平均特征 ∑k Xk*Pk    连续型 ∫∞ |x| * f(x)dx
# 二项分布的期望 E = np
# 泊松分布的期望 E = λ
# 指数分布的期望 E = 1/λ
# 正太分布的期望 E = u
# 随机函数的期望 E(g(x)) = ∑∞ g(Xk)Pk
# 二维随机的期望 E(x,y) = ∑∞ ∑∞ g(xi , yj)Pij    E(x,y) = ∫∞ ∫∞ g(x,y) * f(x,y)dxdy
# E(X + Y) = E(X) + E(Y)
# E(X * Y) = E(X) * E(Y)    X和Y相互独立

# 方差 衡量随机变量取值的波动程度，离散程度
# 计算公式 D(x) = E[x - E(x)]^2  = ∫∞ [x - E(x)]^2 * f(x) dx
# 性质     D(x) = E(x^2) - [E(x)]^2
# X Y 独立 D(X + Y) = D(X) + D(Y)
# 二项分布的方差 D = np(1-p)
# 泊松分布的方差 D = λ
# 指数分布的方差 D = 1/λ^2
# 正太分布的方差 D = σ^2

# 切比雪夫不等式 P(| X - E(X) | ≥ ε) ≤ D(X)/ε2   有上界，根据指定的概率求ε

# 协方差 两个变量总体误差的期望，描述X和Y的相关程度，他们是不是有联系，大于0为正相关、小于0为负相关
# 计算公式 Cov(X，Y)=E[(X-E(X))*(Y-E(Y))] = E(XY) -E(X)*E(Y) 
# 独立的两个随机变量，他们的协方差为0， 协方差为0，表示XY不相关，但不一定独立
# D(X+Y)=D(X)+D(Y)+2Cov(X，Y)
# Cov(X1+X2，Y)=Cov(X1，Y)+Cov(X2，Y)
# 数值来看，协方差的数值越大，两个变量同向程度也就越大

# 相关系数 相关系数是用以反映变量之间相关关系密切程度的统计指标
#  | ρXY | = Cov(X，Y)/√￣D(X)*√￣D(Y)  相关系数ρXY取值在-1到1之间，ρXY = 0时，称X,Y不相关； 
# | ρXY | = 1时，称X,Y完全相关，此时，X,Y之间具有线性函数关系； | ρXY | < 1时，X的变动引起Y的部分变动，ρXY的绝对值越大，X的变动引起Y的变动就越大，
# | ρXY | > 0.8时称为高度相关，当 | ρXY | < 0.3时称为低度相关，其它时候为中度相关

# 矩 
# k阶原点距  Ak= E(X^k) 
# k阶中心距  Bk= E[X-E(X)]^k

# 协方差矩阵 每个元素是各个向量元素之间的协方差

# 大数定律  用数学证明了一个习以为常的道理，当样本数量巨大时，样本的均值等于真实的均值
# 切比雪夫大数定律  limit n->∞ P{|1/n∑Xk - 1/n*∑E(Xk)| < ε} = 1  随着样本容量n的增加，样本平均数将接近于总体平均数
# 伯努利大数定律    当实验次数很大时，可以用事件发生的频率来代替事件的概率
# 辛钦大数定律      独立同分布的切比雪夫大数定律

# 中心极限定律与大数定理
# 
# 中心极限定律--独立同分布   讨论的是随机变量的序列或分布
# 大量相互独立的随机变量，其均值（或者和）的分布以正态分布为极限（意思就是当满足某些条件的时候，
# 比如Sample Size比较大，采样次数区域无穷大的时候，就越接近正态分布）。
# 而这个定理amazing的地方在于，无论是什么分布的随机变量，都满足这个定理。
# 这个正态分布的均值u 和投掷奇形怪状骰子并记录朝上的点数这个随机过程的均值是一！样！的！

# 棣莫佛－拉普拉斯（de Movire - Laplace）中心极限定理
# 服从二项分布的随机变量序列的中心极限定理。它指出，参数为n, p的二项分布以np为均值、np(1-p)为方差的正态分布为极限

# 大数定理
#    简单的可以描述为，如果有一个随机变量X，你不断的观察并且采样这个随机变量，得到了n个采样值，X_{1} , X_{2} , X_{3}....X_{n}，
#    然后求得这n个采样值得平均值-{X_{n}} ，当n趋向于正无穷的时候，这个平均值就收敛于这个随机变量X的期望。


# 样本均值 ￣X = 1/n * ∑ Xi  也称为一阶原点
# 样本方差 S^2 = 1/(n-1) * ∑(X - ￣X)^2
# 抽样分布 抽样所得到的每一个样本可以计算一个平均数，全部可能的样本都被抽取后可以得到许多平均数，由平均数构成的新总体的分布，称为平均数的抽样分布

# 参数估计
# 点估计 借助于总体的一个样本，构造适当的样本函数来估计总体S未知参数的值的问题称为参数的点估计问题. 包括矩估计和最大似然估计两种方法
#       矩估计 最简单的矩估计法是用一阶样本原点矩来估计总体的期望而用二阶样本中心矩来估计总体的方差。 不需要事先知道分布的类型，直接根据样本的期望和方差算参数
#       最大似然估计 依据极大似然估计原理概论大的事件更容易出现，关键是构造似然函数来求极值
#       最大似然函数就是多个样本的联合概率密度相乘，具体的求解方法是，对似然函数取对数，然后对参数求导。
#       在已知试验结果（即是样本）的情况下，用来估计满足这些样本分布的参数，把可能性最大的那个参数clip_image002作为真实clip_image004的参数估计。


# 概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。
# 似然函数可以理解为条件概率的逆反。已知有事件A发生，运用似然函数L(B|A)，我们估计参数B的可能性。
# 对于为什么要对似然函数取对数，是因为许多概率函数都是


"""
                                信息论
"""

# 熵 H(X) = -∑P(x)*log(P(x)) = ∑P(x)1/log(P(x))   系统越有序，信息熵越小

# 联合熵H(x,y)

# 条件熵H(x|y) = -∑P(x,y)*log(P(y|x))   = H(x,y) = H(x)

# 交叉熵D(P||Q) 表示当用概率分布Q来拟合真实分布P时，产生的信息损耗   D(P||Q) = ∑P(x)log(P(x)/Q(x))

# 最大熵模型，在所有可能的概率分布中，熵最大的模型是最好的模型。 在满足约束条件的模型集合中选取熵最大的模型。
# 0 <= H(X) <= log|X|


"""
                                数值计算
"""
# NP问题
# P类问题 以多项式时间的确定算法来对问题进行判定和求解，算法的每一个运行状态都是唯一的，并最终确定一个唯一的结果---最优解
# N类问题 用多项式时间的非确定算法来对问题进行判定和求解

# 迭代计算 

# 最优化 目标函数f(x)的最优解，在满足一定的条件下


"""
                                高等数学
"""

# 梯度下降法     在目标函数是非线性的情况下，按照哪个方向迭代求解误差的收敛速度会最快呢？答案就是梯度方向
# 梯度就是函数的导数方向。根据导数的定义，函数f(x) 的导函数就是目标函数在x上的变化率
# gradf(x,y,z) = (dx, dy, dz)
# 负梯度方向是f(x,y,z) 减小最快的方向

# 随机梯度下降法
# 因为批量梯度下降法每次都使用全部数据，一旦到了某个局部极小值点可能就停止更新了；
# 而随机梯度法由于每次都是随机取部分数据，所以就算局部极小值点，在下一步也还是可以跳出


# 牛顿法  求解无约束最优化问题常用的方法，最大的优点是收敛速度快
# 从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，
# 梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大　
# 所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。

# 牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面
# 牛顿法求解     将目标函数f(x)  在x_{k} 处进行二阶泰勒展开，


# 对数 log   a^x = N, x = loga N  
# 对数求导  loga x' = 1/x*loga e  ln 表示以e为底， loge 3  表示 ln3 。 lne = 1
#           lnx = 1/x

# 指数 a^x
# 指数求导 (a^x)' = a^x*lna   

# (e^x)' = e^x
# (lnx)' = 1/x 






















