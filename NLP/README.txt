NLP，让机器能够理解人类的语言，主要有以下几个方向

1、句法语义分析：对于给定的句子进行分词、词性标记

2、信息抽取：抽取文本中的重要信息，比如时间、地点、人物、结果、日期、货币等

3、文本挖掘：基于统计学习处理，包括文本分类、信息抽取、情感分析等

4、机器翻译：将源语言翻译成另一门语言

5、信息检索 ：检索大规模文档，查找匹配的文档

6、问答系统：根据问题，给出一个答案

7、对话系统：更用户对话、聊天


NLP的一些工具和python学习库

工具：
CRF++ 分词技术的工具

GIZA++ 是词语对齐的重要开源工具

Word2Vec 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，简单、高效


学习库:

1、NLTK 应用最多，文本分类(classification)、文本标记(tokenization)、词干提取(stemming)、词性标记(tagging)、语义分析(parsing)和语义推理(semantic reasoning)

2、Pattern 具有用于词性标注(part-of-speech taggers)、n-gram搜索、情感分析和WordNet的一系列工具。它还支持矢量空间建模、聚类分析以及支持向量机。

3、Gensim 提供了对大型语料库的主题建模、文件索引、相似度检索的功能。它可以处理大于RAM内存的数据

4、spacy 商业的开源软件，速度最快，领域内最先进的自然语言处理工具

5、scikit-learn 已支持文本分类
 
6、TextBlob 处理文本数据的一个Python库

7、PyNLPl 可用于n-gram计算、频率列表和分布、语言建模。

8、Polyglot 是一个支持海量多语言的自然语言处理工具


算法:
HMM 隐马尔科夫链
LDA 线性判别式分析
SVM 支持向量机算法
TF-IDF 资讯检索与资讯探勘的常用加权技术
LSA 潜在语义分析
word2vec 同时也是一种模型


论文
ACL学术论文
ICML会议
CVPR会议











==================================================== 机器学习相关

------------------------------------- 激活函数

激活函数：一般为非线性函数。定义了神经元的输出的映射。神经元通过激活函数把特征保留并映射出来（保留特征，去除一些数据中是的冗余）


作用：将权值结果转化成分类结果，引入激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。比如非常多的情况不能用线性函数进行准确分类。

应用：
1、逻辑回归
2、神经网络

代表：

线性函数： f(x) = w *x +b

阈值函数： f(x) = 1 x>=c, 0 x<c

S形函数(sigmoid)：  f(x) = 1/1+e^-ax    [0, 1]  目前很少用
    1、经过[0,0.5]。但是在饱和区间[梯度接近0]内，梯度几乎为0(不变化)。在BP算法中，这个区间的神经元输出为0，所以没有权重到数据输出了，称为梯度消失。
    不输出该神经元没有贡献，导致网络不学习。
    2、不是关于中心原点对称的，影响梯度下降。在反向传播过程中，要么全是正数，要么全是负数，导致更新时成z字形。 tanh，没有这个问题，所以要受欢迎。
    
    


双曲正切函数：tanh(x) = (e^x - e ^-x)/(e^-x + e^x)   [-1, 1]

-----------上面两个函数，对输入经过归一化[-1, 1]，防止饱和, 因为在[-1,1]区间内，函数倒数比较大。



ReLu函数：修正线性单元，f(x) = max(0, x) 
    1、导数在x > c时 为常数，这就不会产生梯度弥散(导数值很广泛)，同时x < c时为零，神经元在这个区域内不会进行训练（不被激活），即稀疏性
    2、稀疏矩阵 ，矩阵的0元素大于非0元素，因为该函数具有稀疏性，所以可以用稀疏矩阵来表示数据，减少数据冗余，提取特征，因此绝大多数CNN训练中采用ReLu函数。
    
    生物稀疏：研究发现神经元被同时激活只有1%-4%。这说明神经元的稀疏性。经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，
            这样可以提高学习的精度，更好更快地提取稀疏特征。
    这个角度来看，在经验规则的初始化W之后，传统的Sigmoid系函数同时近乎有一半的神经元被激活，这不符合神经科学的研究，而且会给深度网络训练带来巨大问题。
    从而sigmoid函数失去稀疏性。
    
    因此ReLu成了这个模型的最大赢家。
    
    优势：训练时加速。阈值激活，而不需要指数运算，提升运算能力。
    劣势：比较脆弱，可能死掉。因此需要合理设置学习率，会降低这种情况的发生概率
    
    稀疏应用：
    1、信息是相互耦合而影响的，如果够解开特征间缠绕的复杂关系，转换为稀疏特征，那么特征就有了鲁棒性（去掉了无关的噪声）。
    2、线性可分，稀疏特征被移到了一个较为纯净的低维流形面上，稀疏矩阵能够提取特征，从稠密的信息中分解出来，获取本质
    
    
MaxOut函数： 解决了ReLu死掉的问题，但是每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。
            MaxOut相当于在输入和输出之间增加了一层中间层，输入取中间层输出的最大值。
            如果这个中间层MaxOut有k个神经元，那么参数就多了k倍，sigmoid函数只表示一个神经元
            output = K.max(K.dot(X, self.W) + self.b, axis=1)#maxout激活函数
            maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合
            分段的个数与k值有关
            对于maxout 网络也是一个函数逼近器。如果隐藏层的神经元足够多，那么理论上我们是可以逼近任意的函数的
            
            

softmax函数： 作用就是对向量进行归一化，生成概率值
            s(i) = e^i/∑e^j  简单来说就是原来输出是3,1,-3通过softmax函数一作用，就映射成为(0,1)的值。用e^x函数来作用
            softmax函数经常用在神经网络的最后一层，作为输出层，进行多分类。
            简单来说，softmax不止是分成0,1类，可以分成多个类。

------------------------------------- 方法 && 模型
------------------------------------监督学习
逻辑回归 (LogisticRegression) 常用的机器学习方法，用于估计某种事物的可能性。比如点击广告的可能性、患病的可能性、购买某种商品的可能性。
    通常逻辑回归是二元分类。
    
    常规步骤是：
        1、寻找h预测函数（决策函数 即hypothesis），可以是激活函数 1/1+e^-ax
        
        2、构造J函数（损失函数、参数求解）； （似然函数，每个样本的联合分布相乘, 为啥？因为最大似然函数是凸函数，可以通过迭代找到全局最优）
            
            似然函数 L(θ)=P(D|θ)=∏P(y|x;θ)=∏g(θTx)y(1?g(θTx))1?y
            取对数 l(θ)=∑ylog g(θTx)+(1?y)log(1?g(θTx))
            
            单样本的损失函数 cost = -log(h0(x)) if y = 1; -log(1-h0(x)) if y = 0
            m个样本的平均损失函数 J = -1/m ∑cost(h(x), y)。 J(θ)=?1/N*l(θ)
            
            为了防止过拟合，加入了惩罚项 θ^2 θ约等于0   
            J = 1/2m ∑(h(x) - y)^2  + λ∑θ^2  λ是正则项系数：
            
        3、想办法使得J函数最小并求得回归参数（θ） （梯度下降法,对θ求偏导）。
            梯度下降法是一种迭代算法：每一步选取使目标函数变化最快的一个方向调整参数的值来逼近最优值。
            步骤如下：
                选择下降方向（梯度方向，J(θ)）
                选择步长，更新参数 θi=θi?1 * αi * J(θi?1)
                重复以上两步直到满足终止条件
                
                梯度函数为?J/?θ=?1/n * ∑i(yi * yi^*)xi + λθ  

支持向量机SVM
---------------------------------------非监督学习
决策树 CART
 
朴素贝叶斯

K-Means 算法

K均值算法

随机森林算法

Huffman树

反向传播 BP

深度学习算法：主要是神经网络算法，依据特征值


---------------------------------------- 规则
1、正则化 对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，
    导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。
    当模型的参数过多时，很容易遇到过拟合的问题。
    
   通常过拟合问题往往源自过多的特征
   
   通常对于过拟合的问题采用人工减少特征或者正则化的方式来处理。
   
   正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。
   J(θ)=?1/N*∑ylogg(θTx)+(1?y)log(1?g(θTx))+<br/>λ||w||p
   
   过拟合的边界曲线

